{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates some use cases for gathering more or less structured data from the internet. We will extract top university rankings, profiles of a scientific community and world bank data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One: University Rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part we will extract structured information from HTML tables and merge them into a single dataframe. The tables are located on the webpage [shanghairanking.com](www.shanghairanking.com/) where the top universities are ranked upon several scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=http://www.shanghairanking.com/ARWU2003.html width=900 height=650></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe src=http://www.shanghairanking.com/ARWU2003.html width=900 height=650></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the necessary packages will be loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from pandas.io import wb\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pickle\n",
    "import pycountry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part is using the main url as a starting point and then identifies all links we have to crawl for the tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.shanghairanking.com/ARWU2003.html',\n",
       " 'http://www.shanghairanking.com/ARWU2004.html',\n",
       " 'http://www.shanghairanking.com/ARWU2005.html',\n",
       " 'http://www.shanghairanking.com/ARWU2006.html',\n",
       " 'http://www.shanghairanking.com/ARWU2007.html',\n",
       " 'http://www.shanghairanking.com/ARWU2008.html',\n",
       " 'http://www.shanghairanking.com/ARWU2009.html',\n",
       " 'http://www.shanghairanking.com/ARWU2010.html',\n",
       " 'http://www.shanghairanking.com/ARWU2011.html',\n",
       " 'http://www.shanghairanking.com/ARWU2012.html',\n",
       " 'http://www.shanghairanking.com/ARWU2013.html',\n",
       " 'http://www.shanghairanking.com/ARWU2014.html']"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 'http://www.shanghairanking.com/'\n",
    "response = urllib2.urlopen(seed)\n",
    "html = response.read()\n",
    "seedsoup = BeautifulSoup(html)\n",
    "url_list = sorted([list(set(seed + link.get('href') \n",
    "           for link in seedsoup.find_all('a', href=re.compile(r'^ARWU[0-9]{4}'))))][0])\n",
    "url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a function to automate the table extraction for every url we gathered above. Extracting the raw tabular data is pretty easy. The hard part is readjusting the dataframe to assure data consistency over all years. Therefore, several steps like renaming and conversion from string to numeric values are included in the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_rankings(urls):\n",
    "    rankings  = pd.DataFrame()\n",
    "    \n",
    "    # reading in every url in the list created above\n",
    "    for url in urls:\n",
    "        try:\n",
    "            html = urllib2.urlopen(url).read()\n",
    "        except:\n",
    "            time.sleep(30)\n",
    "            html = urllib2.urlopen(url).read()\n",
    "        \n",
    "        # as the country flags are not contained in the tables, we will extract the country\n",
    "        # from the flag image url for each university\n",
    "        html_flags = re.sub(r'(<img.*flag/)(.*)(.png.*</a>)', r'<div>\\2</div>', html)\n",
    "        \n",
    "        df = pd.read_html(html_flags, header=0)[0]\n",
    "        year = re.findall(r'[0-9]{4}', url)[0]\n",
    "        \n",
    "        #several adjustments for data consistency\n",
    "        if year == '2003':\n",
    "            df.rename(columns={'Score on  Nobel  HiCi  N&S PUB  Faculty':'Nobel', \n",
    "                               'Unnamed: 5': 'HiCi', 'Unnamed: 6': 'N&S', \n",
    "                               'Unnamed: 7': 'PUB', \"Unnamed: 8\": \"Faculty\"},\n",
    "                                  inplace=True)\n",
    "            df['Total Score'] = df['Total Score'].convert_objects(convert_numeric=True)\n",
    "        elif year == '2004':\n",
    "            df.rename(columns={'Score on  Alumni  Award  HiCi  N&S PUB  PCP':'Alumni', \n",
    "                               'Unnamed: 5': 'Award', 'Unnamed: 6': 'HiCi', \n",
    "                               'Unnamed: 7': 'N&S', \"Unnamed: 8\": \"PUB\", \"Unnamed: 9\" : \"PCP\" },\n",
    "                              inplace=True)\n",
    "            df['Total Score'] = df['Total Score'].convert_objects(convert_numeric=True)\n",
    "        else: \n",
    "            df.rename(columns={'Score on  Alumni  Award  HiCi  N&S  PUB  PCP':'Alumni', \n",
    "                               'Unnamed: 6': 'Award', 'Unnamed: 7': 'HiCi', \n",
    "                               'Unnamed: 8': 'N&S', \"Unnamed: 9\": \"PUB\",\n",
    "                               \"Unnamed: 10\" : \"PCP\", \"Institution*\": \"Institution\" },\n",
    "                              inplace=True) \n",
    "            df['Total Score'] = df['Total Score'].convert_objects(convert_numeric=True)\n",
    "            df['National Rank'] = df['National Rank'].convert_objects(convert_numeric=True)            \n",
    "            \n",
    "        df.rename(columns={'Alumni':'Alumni' + year , 'Award': 'Award'  + year, \n",
    "                           'Nobel': 'Nobel'  + year, 'HiCi': 'HiCi'  + year, \n",
    "                           'N&S': 'N&S'  + year,  'Faculty': 'Faculty'  + year,\n",
    "                           'PUB': 'PUB'  + year,  'PCP': 'PCP'  + year,  \n",
    "                           \"Country  /Region\": \"Country/Region\"  + year,\n",
    "                            \"Total Score\" : \"Total Score\"  + year,\n",
    "                            \"National Rank\": \"National Rank\"  + year,\n",
    "                            \"World Rank\": \"World Rank\"  + year}, inplace=True) \n",
    "        df.set_index('Institution', inplace=True)\n",
    "        \n",
    "        # this code is merging all data frames using university names as identifiers\n",
    "        if len(rankings) == 0:\n",
    "            rankings = df\n",
    "        else:\n",
    "            rankings = pd.merge(rankings, df, how='outer', \n",
    "                       left_index=\"True\", right_index=\"True\")\n",
    "    return rankings\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the function can be applied to create the combined dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = get_rankings(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python can handle almost every data format. In this case we will write the output to an Excel-Spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('rankings.xlsx')\n",
    "df[sorted(df.columns)].to_excel(writer,'Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the spreadsheet, rows represent all universities and columns the information for every year. The sheet can be downloaded via the followling link:\n",
    "[Shanghai-Rankings](https://www.dropbox.com/s/p1wli8cok83vv9j/rankings.xlsx?dl=1).    \n",
    "\n",
    "Note however that the sheet is not 100% accurate as there are still some inconsistencies within the shanghai-data. This is actually a good example for real life data extraction: it is messy!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two: Leopoldina Member Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will extract information from members of the scientific community [Leopoldina](http://www.leopoldina.org). This time the data are not structured within a table and have to be collected within single webpages for each member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=http://www.leopoldina.org width=900 height=650></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe src=http://www.leopoldina.org width=900 height=650></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting off we again determine the seed page to start crawling url's. There appear to be 1534 member profiles (at the time of writing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 'http://www.leopoldina.org/de/mitglieder/mitgliederverzeichnis/itemsperpage/100/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example for a single member profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=http://www.leopoldina.org/de/mitglieder/mitgliederverzeichnis/member/1141/ width=900 height=650></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe src=http://www.leopoldina.org/de/mitglieder/mitgliederverzeichnis/member/1141/ \\\n",
    "width=900 height=650></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all 1534 member profiles are listed on one single url though.\n",
    "By inspecting the html code of the seed page, we can identify the structure for each subpage containing the member url's. After that, we create a list with every member url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1532"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leopoldina_urls = []\n",
    "for i in xrange(1,17):\n",
    "    url = seed + 'page/' + str(i)\n",
    "    response = urllib2.urlopen(url)\n",
    "    html = response.read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    for e in soup.find_all(class_='more-link'):\n",
    "        link = e.find('a', href=re.compile(r'mitgliederverzeichnis/member/.'))\n",
    "        if str(type(link)) == \"<class 'bs4.element.Tag'>\":\n",
    "            leopoldina_urls.append(seed[:26] + link.get('href'))\n",
    "            \n",
    "len(scientists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two links appear to be broken, so we are left with 1532 web pages to crawl. The following function enables iterating over each member profile. We are interested in several attributes like *country*, *career* and *memberships* as well as PDF-Files containg CV's for every member. \n",
    "Again, real life data are messy and finding the attributes for all members requires flexible code. Therefore, different wordings of attributes are considered and possible error messages (e.g. if there is no PDF.File for a specific member) will be catched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_scientists(url_list):\n",
    "    leopoldina = {}\n",
    "    pdfs = 0\n",
    "    with open('logfile.log', 'wb') as f:\n",
    "\n",
    "        for link in url_list:\n",
    "            f.write(link.encode('utf-8') + \" wird verarbeitet.. \\n\") \n",
    "            try:\n",
    "                html = urllib2.urlopen(link).read()\n",
    "            except:\n",
    "                time.sleep(10)\n",
    "                html = urllib2.urlopen(link).read()\n",
    "            soup = BeautifulSoup(html)\n",
    "\n",
    "            name = soup.find(class_='text').find('h1').text\n",
    "            leopoldina[name] = {}\n",
    "            cols = soup.find_all(class_='col2')\n",
    "            leopoldina[name]['wahljahr'] = cols[0].text\n",
    "            leopoldina[name]['sektion'] = cols[1].text\n",
    "            leopoldina[name]['stadt'] = cols[2].text\n",
    "            leopoldina[name]['land'] = cols[3].text\n",
    "\n",
    "            if soup.find('a', class_='downloadLinkZeile'):\n",
    "                cv_url = soup.find('a', class_='downloadLinkZeile').get('href')\n",
    "                try:\n",
    "                    pdf_file = urllib2.urlopen(seed[:26] + cv_url).read()\n",
    "                except:\n",
    "                    time.sleep(10)\n",
    "                    pdf_file = urllib2.urlopen(seed[:26] + cv_url).read()\n",
    "                with open(name + '_cv.pdf' , 'wb') as pdf:\n",
    "                    pdf.write(pdf_file)\n",
    "                pdfs += 1\n",
    "                f.write('PDF gefunden und gespeichert. \\n')\n",
    "            if soup.find('h3', text='Forschung'):\n",
    "                forschung = soup.find('h3', text='Forschung')\n",
    "                forschung_string = \"\"\n",
    "                forschung_text = forschung.findNextSiblings()\n",
    "                for text in forschung_text:\n",
    "                    forschung_string += text.text\n",
    "                leopoldina[name]['forschung'] = forschung_string\n",
    "          \n",
    "            for i in ['Werdegang', 'Career']:  \n",
    "                if soup.find('h3', text=i):\n",
    "                    werdegang = soup.find('h3', text=i)\n",
    "                    werdegang_string = \"\"\n",
    "                    werdegang_text = werdegang.findNext()\n",
    "                    try:\n",
    "                        for text in werdegang_text:\n",
    "                            werdegang_string += unicode(text.text)\n",
    "                            werdegang_string += \" \\n\"\n",
    "                    except AttributeError:\n",
    "                        werdegang_string = werdegang_text.text\n",
    "                    leopoldina[name]['werdegang'] = werdegang_string\n",
    "                    break\n",
    "                    \n",
    "            for i in ['Auszeichnungen und Mitgliedschaften',\n",
    "                      'Auszeichnungen und Mitgliedschaften (Auswahl)', \n",
    "                      'Honours and Memberships']:\n",
    "                if soup.find('h3', text=i):\n",
    "                    am = soup.find('h3', text=i)\n",
    "                    am_string = \"\"\n",
    "                    am_text = am.findNext()\n",
    "                    try:\n",
    "                        for text in am_text:\n",
    "                            am_string += unicode(text.text)\n",
    "                            am_string += \" \\n\"\n",
    "                    except AttributeError:\n",
    "                        am_string = am_text.text\n",
    "                    leopoldina[name]['auszeichnungen/mitgliedschaften'] = am_string\n",
    "                    break\n",
    "\n",
    "            f.write(name.encode('utf-8') + \" gespeichert. \\n\\n\") \n",
    "        f.write(\"\\nInsgesamt \" + unicode(len(leopoldina.keys())) + \n",
    "                \" Profile und \" + str(pdfs) + \" PDF's gespeichert.\")\n",
    "        with open('leopoldina.p', 'wb') as d:\n",
    "            pickle.dump(test, d)\n",
    "        return leopoldina\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can apply the function and crawl every web page to extract member information (this takes some time!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "leopoldina = get_scientists(leopoldina_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1530"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leopoldina)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, we are left with 1530 member profiles as 2 links again appeared to be broken. The information are stored within a python dictionary but could be extracted with a single line of code. A Log-File has been written to the current working directory, containing the crawling progress. Also, if available, a CV in PDF format is automatically downloaded and named after the corresponding scientist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output shows data for the scientist *Dorairajan Balasubramanian*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sektion: \n",
      "Ophthalmologie, Oto-Rhino-Laryngologie, Stomatologie \n",
      "\n",
      "land: \n",
      "Indien \n",
      "\n",
      "wahljahr: \n",
      "2009 \n",
      "\n",
      "forschung: \n",
      "Dorairajan Balasubramanian is distinguished for his important contributions to the understandig of the basic biological processes involved in some eye diseases, notably cataract and glaucoma. His expertise in biophysical chemistry and molecular biology have enabled him to \r",
      "(a) identify the origin and chemical identities of several chromophores and pigments that accumulate in the aging and cataractous human lens; (b) to show how some of these accumulants contribute further to the covalent damage of the lens proteins, through oxidative and cross-linking mechanisms; (c) to suggest the mechanistic link between smoke inhalation and cataractogenesis; (d) to evaluate the ability of some plant natural products in delaying/preventing cataract; and (e) to do molecular functional analysis of mutant crystallins that are seen in congenital cataracts in children, and show how these molecules form scattering particles in situ in lens cells. \r",
      "More recently, he has been able to suggest one of the functions of the glaucoma-associated protein optineurin and how mutations in it lead to retinal ganglion cell death. \n",
      "\n",
      "stadt: \n",
      "Hyderabad \n",
      "\n",
      "auszeichnungen/mitgliedschaften: \n",
      "Elected Fellow of Indian Academy of Sciences, Bangalore, India \n",
      "Elected Fellow of Indian National Science Academy, New Delhi \n",
      "Elected Fellow of National Academy of Sciences India, Allahabad \n",
      "Elected Fellow, American Association for the Advancement of Science (AAAS), Washington DC, USA \n",
      "Elected Fellow, Academy of Sciences of the Developing World (earlier called the Third World Academy of Sciences, TWAS), Trieste, Italy \n",
      "Elected Fellow, Mauritius Academy of Science and Technology \n",
      "Elected Fellow, National Academy of Sciences Leopoldina, Germany \n",
      "\n",
      "\n",
      "werdegang: \n",
      "2007-2010 President of the Indian Academy of Sciences, Bangalore, India  \n",
      "since 1998 Director of Research, L. V. Prasad Eye Institute, Hyderabad, India \n",
      "1982-1998 Deputy Director and Director, centre for Cellular and Molecular Biology, Hyderabad, India \n",
      "1977-1982 Professor and Dean, School of Chemistry, University of Hyderabad, Hyderabad, India  \n",
      "1967-1977 Lecture and Assistant Professor in Chemistry, Indian Institute of Technology, Kanpur, India  \n",
      "1965-1966 Post-doctoral Research (as a Jane Coffin Foundation Fellow) at the Department of Biochemistry, University of Minnesota Medical School, Minneapolis, USA \n",
      "1965 Ph. D. degree in Chemistry, from Columbia University, New York, USA  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in leopoldina['Dorairajan Balasubramanian'].items():\n",
    "    print k+\":\", \"\\n\", v, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to store python objects on disk by using the ````Pickle```` package. An example file for the member profiles can be downloaded here:    \n",
    "[Leopold-Dictionary](https://www.dropbox.com/s/3icbnrxp10g7fx8/leopoldina.p?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Log-File for the crawling can be downloaded here:   \n",
    "[Log-File](https://www.dropbox.com/s/4vw0oxiile1tvjc/logfile.log?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three: World Bank API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last part we will make use of a direct connection to the World Bank API to extract data. Accessing API's generally is much more convenient then web scraping, as the data are highly structured and you don't have to deal with problems like messy html code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas io features, we can search the world bank api for any expressions or (if already known) variable names. Here we are interested in high technology exports and their share in GDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8342</th>\n",
       "      <td>    TX.VAL.TECH.CD</td>\n",
       "      <td>             High-technology exports (current US$)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8343</th>\n",
       "      <td> TX.VAL.TECH.MF.ZS</td>\n",
       "      <td> High-technology exports (% of manufactured exp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               name\n",
       "8342     TX.VAL.TECH.CD              High-technology exports (current US$)\n",
       "8343  TX.VAL.TECH.MF.ZS  High-technology exports (% of manufactured exp..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb.search('high-technology').iloc[:,:2][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>  5.51.01.10.gdp</td>\n",
       "      <td>     Per capita GDP growth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td> 6.0.GDP_current</td>\n",
       "      <td>         GDP (current US$)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>  6.0.GDP_growth</td>\n",
       "      <td>     GDP growth (annual %)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>     6.0.GDP_usd</td>\n",
       "      <td>   GDP (constant 2005 US$)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>       6.0.GDPpc</td>\n",
       "      <td> GDP per capita (2011 US$)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                       name\n",
       "694   5.51.01.10.gdp      Per capita GDP growth\n",
       "696  6.0.GDP_current          GDP (current US$)\n",
       "697   6.0.GDP_growth      GDP growth (annual %)\n",
       "698      6.0.GDP_usd    GDP (constant 2005 US$)\n",
       "699        6.0.GDPpc  GDP per capita (2011 US$)"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb.search('gdp').iloc[:,:2][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After identifying the target variables, we can easily download the corresponding data directly from the world bank databank and specify the time span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat = wb.download(indicator=['TX.VAL.TECH.CD', '6.0.GDP_current']\n",
    "                  , start=2005, end=2015, country =['all']).dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a Data-Frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>TX.VAL.TECH.CD</th>\n",
       "      <th>6.0.GDP_current</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Argentina</th>\n",
       "      <th>2012</th>\n",
       "      <td> 1945836328</td>\n",
       "      <td> 6.031530e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td> 1978069865</td>\n",
       "      <td> 5.577273e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td> 1648289776</td>\n",
       "      <td> 4.627038e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td> 1546820304</td>\n",
       "      <td> 3.784962e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td> 1948517138</td>\n",
       "      <td> 4.060037e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                TX.VAL.TECH.CD  6.0.GDP_current\n",
       "country   year                                 \n",
       "Argentina 2012      1945836328     6.031530e+11\n",
       "          2011      1978069865     5.577273e+11\n",
       "          2010      1648289776     4.627038e+11\n",
       "          2009      1546820304     3.784962e+11\n",
       "          2008      1948517138     4.060037e+11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the high technology share in GDP only requires a simple division of the two columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>TX.VAL.TECH.CD</th>\n",
       "      <th>6.0.GDP_current</th>\n",
       "      <th>share</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Panama</th>\n",
       "      <th>2011</th>\n",
       "      <td>  4769250391</td>\n",
       "      <td> 3.327050e+10</td>\n",
       "      <td> 0.143348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">Costa Rica</th>\n",
       "      <th>2007</th>\n",
       "      <td>  2530343873</td>\n",
       "      <td> 2.632200e+10</td>\n",
       "      <td> 0.096130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>  2090631242</td>\n",
       "      <td> 2.252646e+10</td>\n",
       "      <td> 0.092808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>  1776832950</td>\n",
       "      <td> 1.996489e+10</td>\n",
       "      <td> 0.088998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>  2426063976</td>\n",
       "      <td> 2.983117e+10</td>\n",
       "      <td> 0.081326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>  1791400685</td>\n",
       "      <td> 2.938269e+10</td>\n",
       "      <td> 0.060968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>  2511568834</td>\n",
       "      <td> 4.123730e+10</td>\n",
       "      <td> 0.060905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>  2193335855</td>\n",
       "      <td> 3.629833e+10</td>\n",
       "      <td> 0.060425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>  2719083321</td>\n",
       "      <td> 4.537479e+10</td>\n",
       "      <td> 0.059925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mexico</th>\n",
       "      <th>2005</th>\n",
       "      <td> 32400251888</td>\n",
       "      <td> 8.663465e+11</td>\n",
       "      <td> 0.037399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 TX.VAL.TECH.CD  6.0.GDP_current     share\n",
       "country    year                                           \n",
       "Panama     2011      4769250391     3.327050e+10  0.143348\n",
       "Costa Rica 2007      2530343873     2.632200e+10  0.096130\n",
       "           2006      2090631242     2.252646e+10  0.092808\n",
       "           2005      1776832950     1.996489e+10  0.088998\n",
       "           2008      2426063976     2.983117e+10  0.081326\n",
       "           2009      1791400685     2.938269e+10  0.060968\n",
       "           2011      2511568834     4.123730e+10  0.060905\n",
       "           2010      2193335855     3.629833e+10  0.060425\n",
       "           2012      2719083321     4.537479e+10  0.059925\n",
       "Mexico     2005     32400251888     8.663465e+11  0.037399"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['share'] = dat['TX.VAL.TECH.CD'] / dat['6.0.GDP_current']\n",
    "dat.sort(columns='share', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, accessing world bank information with Python is pretty straight forward. However, not every API is as easy to handle and well documented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we might want to limit an API request to only contain data for specific countries. Of course, we could just pass a list of countries. But what if there are many countries? And what if the list of countries has to be adapted for the API request?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we are interested only in countries which appear in the shanghai rankings from *Part One*. The strings containing the image-url's are not structured at all and again a perfect example of messy real life data. First, we extract all strings from the DataFrame we created before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries = sorted([list(set(val for col in df.filter(like=\"Country\")\n",
    "                                 for val in df[col].dropna()))][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding a proper format, the World Bank API for example accepts  ISO-Country-Codes. So we have to clean up the list of countries by using several regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries_clean = []\n",
    "for c in countries:\n",
    "    c  = re.sub(r'(China-)(.*)', r'\\2', c)\n",
    "    c  = re.sub(r'(.*),(.*)', r'\\1', c)\n",
    "    c  = re.sub(r'([A-Z].*[a-z])([A-Z].*)', r'\\1 \\2', c)\n",
    "    c = c.replace('Taiwan', 'Taiwan, Province of China')\n",
    "    c = c.replace('Iran', 'Iran, Islamic Republic of')\n",
    "    c = c.replace('Russia', 'Russian Federation')\n",
    "    c = c.replace('Czech', 'Czech Republic')\n",
    "    c = c.replace('South Korea', 'Korea, Republic of')\n",
    "    if len(c) > 3:   \n",
    "        countries_clean.append(c)\n",
    "countries_clean = sorted(list(set(countries_clean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we convert the country strings into ISO codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "countrie_codes = {}\n",
    "for country in pycountry.countries:\n",
    "    countrie_codes[country.name] = country.alpha3\n",
    "codes = [countrie_codes.get(country, 'Unknown code') for country in countries_clean]\n",
    "print len(codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall we are left with a list of 45 codes. The first 10 ISO codes and the corresponding country names are printed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singapore SGP\n",
      "Slovenia SVN\n",
      "South Africa ZAF\n",
      "Spain ESP\n",
      "Sweden SWE\n",
      "Switzerland CHE\n",
      "Taiwan, Province of China TWN\n",
      "Turkey TUR\n",
      "United Kingdom GBR\n",
      "United States USA\n"
     ]
    }
   ],
   "source": [
    "for country, code in zip(countries_clean, codes)[-10:]:\n",
    "    print country, code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can make use of the list for another API request. This time we are interested in charges for intellectual property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>      BM.GSR.ROYL.CD</td>\n",
       "      <td> Charges for the use of intellectual property, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>      BX.GSR.ROYL.CD</td>\n",
       "      <td> Charges for the use of intellectual property, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5336</th>\n",
       "      <td> IC.PRP.COST.PROP.ZS</td>\n",
       "      <td> Cost of registering property (% of property va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5337</th>\n",
       "      <td>         IC.PRP.DURS</td>\n",
       "      <td>         Time required to register property (days)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5338</th>\n",
       "      <td>         IC.PRP.PROC</td>\n",
       "      <td>          Procedures to register property (number)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                               name\n",
       "1513       BM.GSR.ROYL.CD  Charges for the use of intellectual property, ...\n",
       "1594       BX.GSR.ROYL.CD  Charges for the use of intellectual property, ...\n",
       "5336  IC.PRP.COST.PROP.ZS  Cost of registering property (% of property va...\n",
       "5337          IC.PRP.DURS          Time required to register property (days)\n",
       "5338          IC.PRP.PROC           Procedures to register property (number)"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb.search('property').iloc[:,:2][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the download we hand over the list with country codes. For unknown reasons, Taiwan had to be excluded because the API raised errors for this country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_prop = wb.download(indicator='BM.GSR.ROYL.CD', start ='2013', end='2013',\n",
    "                country = [code for code in codes if code != \"TWN\"]).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the result is another DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>BM.GSR.ROYL.CD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>United States</th>\n",
       "      <th>2013</th>\n",
       "      <td> 39016000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Kingdom</th>\n",
       "      <th>2013</th>\n",
       "      <td>  9036715541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turkey</th>\n",
       "      <th>2013</th>\n",
       "      <td>   785000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sweden</th>\n",
       "      <th>2013</th>\n",
       "      <td>  2246965121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spain</th>\n",
       "      <th>2013</th>\n",
       "      <td>  2096370149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     BM.GSR.ROYL.CD\n",
       "country        year                \n",
       "United States  2013     39016000000\n",
       "United Kingdom 2013      9036715541\n",
       "Turkey         2013       785000000\n",
       "Sweden         2013      2246965121\n",
       "Spain          2013      2096370149"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prop.sort(ascending=False).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
